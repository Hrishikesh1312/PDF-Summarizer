# ğŸ§  PDF Summarizer â€” using Ollama + PyQt5

A simple, privacy-focused desktop app to summarize one or more PDF files using LLMs â€” all running locally on your machine.

This project uses:

- ğŸ§  [Ollama](https://ollama.com) for local large language model inference
- ğŸ PyQt5 for a modern desktop user interface
- ğŸ“„ Support for **single or batch PDF summarization**
- ğŸŒ™ Toggle between dark mode and light mode
- ğŸ“‚ Export summaries as `.txt` files

---

## âš™ï¸ Requirements

> Ensure you have the following installed:

```bash
# Python dependencies
pip install PyQt5 fitz reportlab requests

# Ollama (for LLMs)
https://ollama.com/download

# Start the Ollama server
ollama serve
```

---

## ğŸš€ Getting Started

```bash
git clone https://github.com/Hrishikesh1312/PDF-Summarizer
cd PDF-Summarizer

# Start Ollama in a separate terminal
ollama serve

# Run the app
python main.py
```

---

## ğŸ›¡ï¸ Why Local?

This tool **never sends your PDF files or content to the cloud**.

âœ… Your PDFs are read and summarized **entirely on your system**\
âœ… Ollama runs the LLM **locally**, using models like `llama3`\
âœ… No API keys or internet needed after setup

---

## âœ¨ Features

- âœ… Choose summary length (in number of sentences)
- âœ… Dark Mode / Light Mode toggle
- âœ… Export summaries as `.txt`
- âœ… Clean and responsive desktop UI
- âœ… 100% local â€” your documents never leave your device

---

## ğŸ‘¤ Screenshot

Light Theme             |  Dark Theme
:-------------------------:|:-------------------------:
![Screenshot 2025-06-22 135917](https://github.com/user-attachments/assets/e08b1047-f3de-4c6c-872c-7e2cf5b0a594)  |  ![Screenshot 2025-06-22 135932](https://github.com/user-attachments/assets/d45f0039-a317-47e7-a691-0645f58b0ccd)

---

## ğŸ“‚ Folder Structure

```
pdf-summarizer/
â”œâ”€â”€ main.py              # Entry point
â”œâ”€â”€ interface.py           # PyQt5 GUI
â”œâ”€â”€ summarizer.py        # PDF text extraction + Ollama integration
â””â”€â”€ README.md
```

---

## ğŸ§  Customizing the Model

You can change the model used by editing `summarizer.py`:

```python
def summarize_with_ollama(text, summary_length=5, model="llama3"):
```

Try other local models like `mistral`, `gemma`, etc. (see: `ollama list`)

---

## ğŸ“œ License

This project is open source and available under the [MIT License](LICENSE).

---

## ğŸ™Œ Acknowledgments

- [Ollama](https://ollama.com) â€” for blazing fast local LLMs
- [PyMuPDF](https://pymupdf.readthedocs.io/) â€” for fast and accurate PDF parsing
- [PyQt5](https://doc.qt.io/qtforpython/) â€” for beautiful UIs in Python

---

